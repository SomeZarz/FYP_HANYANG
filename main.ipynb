{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a2dcab",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551c7db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEPENDENCY STATUS ===\n",
      " ✓ h5py                 v3.15.1         HDF5 file format support\n",
      " ✓ matplotlib           v3.10.7         Plotting library\n",
      " ✓ memory_profiler      v0.61.0         Memory profiling tool\n",
      " ✓ numpy                v1.26.4         Numerical computing library\n",
      " ✓ pandas               v2.3.3          Data manipulation library\n",
      " ✓ scipy                v1.16.3         Scientific computing library\n",
      " ✓ seaborn              v0.13.2         Statistical visualization library\n",
      " ✓ sklearn              v1.7.2          Scikit-learn ML library\n",
      " ✓ torch                v2.9.1          PyTorch deep learning framework\n",
      " ✓ torchaudio           v2.9.1          PyTorch audio utilities\n",
      " ✓ torchvision          v0.24.1         PyTorch vision utilities\n",
      " ✓ tqdm                 v4.67.1         Progress bar library\n",
      " ✓ yaml                 v6.0.3          Config manipulation library\n",
      "\n",
      "=== GPU/ACCELERATOR STATUS ===\n",
      "Platform: arm64\n",
      "Apple Silicon: Yes\n",
      "\n",
      "PyTorch MPS:\n",
      " Installed: Yes\n",
      " Version: 2.9.1\n",
      " MPS Available: Yes\n",
      " Device: mps\n",
      "\n",
      "Recommended Device: MPS\n",
      "\n",
      "=== HDF5 I/O TEST ===\n",
      "Working: ✓ Yes\n"
     ]
    }
   ],
   "source": [
    "# --- MacOS Validator ---\n",
    "import sys\n",
    "sys.path.insert(0, './setup_validation')\n",
    "\n",
    "from macos_validator import main\n",
    "results = main(run_benchmark=False)\n",
    "\n",
    "#  --- Path Setup ---\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, './src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70955e4c",
   "metadata": {},
   "source": [
    "# Config Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43697f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_profile': 'data_paper', 'dataset_name': 'sample', 'paths': {'datasetdir': 'data', 'csv_pattern': '*.csv', 'extracted_datadir': 'data_extracted', 'hdf5_filename': 'data_paper_sample.h5', 'checkpoint_dir': 'checkpoints'}, 'dataset': {'rated_capacity_ah': 155.0, 'num_cells': 96, 'voltage_window_mv': [3900, 4050]}, 'processing': {'config_file': '{data_profile}.yaml', 'parallel_batch_size': 6}, 'ocv_calibration': {'min_rest_hours': 1.0, 'max_soc_start': 0.6, 'ocv_table_path': None}, 'charging': {'status_value': 3, 'accept_partial_charges': True, 'full_charge_soc_threshold': 0.99, 'full_charge_voltage_v': 4.24}, 'quality_checks': {'min_samples_in_window': 20, 'max_gap_seconds': 60, 'current_bounds_a': [0, 220], 'current_sign_convention': 'physical', 'voltage_smoothing': 'interpolation'}, 'soh_calculation': {'soh_bounds': [0.35, 1.25], 'min_abs_delta_soc': 0.03}}\n"
     ]
    }
   ],
   "source": [
    "from src.utils import load_config\n",
    "config = load_config(\"config.yaml\")\n",
    "print (config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf487cf",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deef18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# In main.ipynb\n",
    "from src.data_extract import extract_all_features\n",
    "from src.utils import load_config\n",
    "import pandas as pd\n",
    "\n",
    "config = load_config(\"config.yaml\")\n",
    "df = pd.read_csv(\"data/sample/vin1.csv\")  # Use one actual file\n",
    "features = extract_all_features(df, config)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db732d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_process import get_vehicle_paths\n",
    "from src.utils import load_config\n",
    "from src.data_extract import extract_all_features\n",
    "import pandas as pd\n",
    "\n",
    "config = load_config(\"config.yaml\")\n",
    "csv_dir = Path(config['paths']['datasetdir']) / config['dataset_name']\n",
    "vehicle_paths = get_vehicle_paths(csv_dir, \"*.csv\")[:5]  # Only first 5\n",
    "\n",
    "for path in vehicle_paths:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing: {path.stem}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df['vehicleid'] = path.stem\n",
    "    \n",
    "    features = extract_all_features(df, config)\n",
    "    if features:\n",
    "        print(f\"✅ SUCCESS: {len(features['soh_labels'])} samples\")\n",
    "    else:\n",
    "        print(f\"❌ FAILED: No valid segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8d47d",
   "metadata": {},
   "source": [
    "# Data Process & H5 Packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c3946",
   "metadata": {},
   "source": [
    "## SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7317b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarz/Code/FYP_HANYANG/fyp_hanyang/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_paper_sample_checkpoint.json\n",
      "Data Profile: data_paper\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 50/50 [01:37<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 5\n",
      "No valid segments: 295\n",
      "Errors: 0\n",
      "Total samples extracted: 8\n",
      "SOH range: 0.518 to 0.832\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Paper Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config_path = \"config.yaml\"\n",
    "extraction_pipeline(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc42974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_sample_sample_checkpoint.json\n",
      "Data Profile: data_sample\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 50/50 [02:08<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 24\n",
      "No valid segments: 276\n",
      "Errors: 0\n",
      "Total samples extracted: 51\n",
      "SOH range: 0.308 to 1.210\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Sample Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config_path = \"config.yaml\"\n",
    "extraction_pipeline(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e29bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_strict_sample_checkpoint.json\n",
      "Data Profile: data_strict\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 3/50 [00:05<01:33,  1.98s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:429: UserWarning: Vehicle vin119: No rest events\n",
      "  warnings.warn(f\"Vehicle {vehicle_id}: No rest events\")\n",
      "Processing batches:  16%|█▌        | 8/50 [00:15<01:22,  1.95s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:499: UserWarning: Failed to process segment: 0\n",
      "  warnings.warn(f\"Failed to process segment: {e}\")\n",
      "Processing batches:  18%|█▊        | 9/50 [00:17<01:20,  1.96s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:499: UserWarning: Failed to process segment: 0\n",
      "  warnings.warn(f\"Failed to process segment: {e}\")\n",
      "Processing batches:  90%|█████████ | 45/50 [01:22<00:09,  1.85s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:429: UserWarning: Vehicle vin75: No rest events\n",
      "  warnings.warn(f\"Vehicle {vehicle_id}: No rest events\")\n",
      "Processing batches: 100%|██████████| 50/50 [01:31<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 1\n",
      "No valid segments: 299\n",
      "Errors: 0\n",
      "Total samples extracted: 2\n",
      "SOH range: 0.512 to 0.515\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Strict Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "extraction_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75d20a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_lenient_sample_checkpoint.json\n",
      "Data Profile: data_lenient\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 20/50 [01:07<01:41,  3.37s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:499: UserWarning: Failed to process segment: 0\n",
      "  warnings.warn(f\"Failed to process segment: {e}\")\n",
      "Processing batches: 100%|██████████| 50/50 [02:42<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 61\n",
      "No valid segments: 239\n",
      "Errors: 0\n",
      "Total samples extracted: 136\n",
      "SOH range: 0.206 to 1.280\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Lenient Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "extraction_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bed3139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_current_sample_checkpoint.json\n",
      "Data Profile: data_current\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 50/50 [01:51<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 12\n",
      "No valid segments: 288\n",
      "Errors: 0\n",
      "Total samples extracted: 22\n",
      "SOH range: 0.353 to 1.072\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Current Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "extraction_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a24a057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_voltage_sample_checkpoint.json\n",
      "Data Profile: data_voltage\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 50/50 [01:57<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 15\n",
      "No valid segments: 285\n",
      "Errors: 0\n",
      "Total samples extracted: 27\n",
      "SOH range: 0.353 to 1.072\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Voltage Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "extraction_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2599f5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/data_temporal_sample_checkpoint.json\n",
      "Data Profile: data_temporal\n",
      "Dataset: sample\n",
      "Found 300 vehicle files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 8/50 [00:16<01:26,  2.05s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:499: UserWarning: Failed to process segment: 0\n",
      "  warnings.warn(f\"Failed to process segment: {e}\")\n",
      "Processing batches:  26%|██▌       | 13/50 [00:25<01:09,  1.87s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:499: UserWarning: Failed to process segment: 0\n",
      "  warnings.warn(f\"Failed to process segment: {e}\")\n",
      "Processing batches:  58%|█████▊    | 29/50 [00:56<00:38,  1.83s/it]/Users/zarz/Code/FYP_HANYANG/src/data_extract.py:499: UserWarning: Failed to process segment: 0\n",
      "  warnings.warn(f\"Failed to process segment: {e}\")\n",
      "Processing batches: 100%|██████████| 50/50 [01:39<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Total vehicles processed: 300\n",
      "Successful extractions: 3\n",
      "No valid segments: 297\n",
      "Errors: 0\n",
      "Total samples extracted: 3\n",
      "SOH range: 0.408 to 0.457\n",
      "==================================================\n",
      "HDF5 file closed safely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Temporal Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "extraction_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Final Sample Feature Extrction --\n",
    "from src.data_process import extraction_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "extraction_pipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8690a4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d57dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import model_pipeline\n",
    "\n",
    "config = \"config.yaml\"\n",
    "trained_model, test_results = model_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_hanyang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
