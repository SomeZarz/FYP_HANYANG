# --- Model Configuration (Paper Specification) ---
model:
  name: "FusionModel"
  architecture: "resnet" # Options: resnet, transformer
  embed_dim: 128
  
  # Sequence processor options
  sequence_processor: "resnet"
  
  # Transformer-specific (if architecture: transformer)
  transformer:
    nhead: 4
    num_layers: 2
    d_model: 64
    seq_len: 151
  
  # Gradient clipping
  grad_clip_norm: null         # or numeric value like 1.0

training:
  optimizer: "adam"
  batch_size: 32
  learning_rate: 1.0e-3
  weight_decay: 1.0e-5
  num_epochs: 100
  patience: 10
  
  # Learning rate schedule (Paper: Supplementary Note 6)
  lr_schedule: "warmup_exponential"
  warmup_epochs: 10             # ← CRITICAL: Currently hard-coded
  warmup_schedule: "linear"     # ← CRITICAL: Currently hard-coded
  decay_factor: 0.95            # ← CRITICAL: Currently hard-coded
  
  # Reproducibility (Paper: "tests repeated ten times")
  n_repeats: 10
